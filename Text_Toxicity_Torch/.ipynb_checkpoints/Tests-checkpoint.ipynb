{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7272c31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class get_data(Dataset):\n",
    "    def __init__(self):\n",
    "        self.data = pd.read_csv(\"train_data_version3.csv\")\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.x = [self.tokenizer(text,\n",
    "                                padding='max_length', max_length= 512, truncation=True,\n",
    "                                return_tensors=\"pt\") for text in list(self.data[\"text\"])]\n",
    "        self.y = list(self.data[\"y\"])\n",
    "        \n",
    "\n",
    "    def get_df(self):\n",
    "        return self.data\n",
    "    def get_len(self):\n",
    "        return self.data.shape\n",
    "    def get_item(self, indx):\n",
    "        return self.data.loc[indx]\n",
    "    def get_summary(self):\n",
    "        return self.data.describe()\n",
    "    def prepare_data(self):\n",
    "        \n",
    "        #self.texts = \n",
    "\n",
    "        train, test = torch.utils.data.random_split([self.x, self.y], [0.7, 0.3])\n",
    "        return train, test\n",
    "\n",
    "\n",
    "        # y_train = torch.tensor(list(y_train), dtype=torch.float32)\n",
    "        # y_test = torch.tensor(list(y_test), dtype=torch.float32)\n",
    "#         train_dl = DataLoader(train, batch_size=16, shuffle=True)\n",
    "#         test_dl = DataLoader(test, batch_size=16, shuffle=False)\n",
    "#         return train_dl, test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "362d81bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67b6acea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.08,\n",
       " 3.58,\n",
       " 0.8,\n",
       " 4.58,\n",
       " 2.08,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 2.08,\n",
       " 3.42,\n",
       " 2.14,\n",
       " 2.14,\n",
       " 2.08,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 4.58,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 4.58,\n",
       " 1.92,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 1.92,\n",
       " 1.92,\n",
       " 1.92,\n",
       " 0.8,\n",
       " 1.28,\n",
       " 2.08,\n",
       " 6.08,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 4.58,\n",
       " 2.08,\n",
       " 2.08,\n",
       " 4.58,\n",
       " 2.08,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 4.58,\n",
       " 0.64,\n",
       " 3.58,\n",
       " 2.08,\n",
       " 0.8,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 4.58,\n",
       " 0.64,\n",
       " 0.16,\n",
       " 4.58,\n",
       " 3.58,\n",
       " 1.92,\n",
       " 2.08,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 0.8,\n",
       " 0.8,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 0.8,\n",
       " 0.16,\n",
       " 1.92,\n",
       " 1.44,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 3.42,\n",
       " 2.08,\n",
       " 0.8,\n",
       " 3.3,\n",
       " 4.58,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 1.92,\n",
       " 3.3,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 3.42,\n",
       " 2.08,\n",
       " 2.08,\n",
       " 6.08,\n",
       " 1.92,\n",
       " 0.64,\n",
       " 1.28,\n",
       " 0.64,\n",
       " 6.08,\n",
       " 2.08,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 1.92,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 0.8,\n",
       " 4.58,\n",
       " 6.08,\n",
       " 2.14,\n",
       " 3.42,\n",
       " 1.92,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 1.92,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 1.44,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 4.58,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 3.42,\n",
       " 3.42,\n",
       " 0.8,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 0.8,\n",
       " 6.08,\n",
       " 2.08,\n",
       " 3.42,\n",
       " 1.92,\n",
       " 4.58,\n",
       " 2.08,\n",
       " 4.58,\n",
       " 0.64,\n",
       " 0.8,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 1.92,\n",
       " 4.42,\n",
       " 0.64,\n",
       " 0.8,\n",
       " 1.28,\n",
       " 4.58,\n",
       " 3.58,\n",
       " 2.08,\n",
       " 3.58,\n",
       " 0.64,\n",
       " 4.58,\n",
       " 1.92,\n",
       " 1.28,\n",
       " 1.5,\n",
       " 1.92,\n",
       " 0.64,\n",
       " 1.92,\n",
       " 2.08,\n",
       " 1.92,\n",
       " 4.58,\n",
       " 0.8,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 0.8,\n",
       " 3.14,\n",
       " 2.78,\n",
       " 2.08,\n",
       " 0.8,\n",
       " 2.08,\n",
       " 3.58,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 3.42,\n",
       " 1.92,\n",
       " 6.08,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 1.92,\n",
       " 2.08,\n",
       " 1.92,\n",
       " 0.8,\n",
       " 4.58,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 1.44,\n",
       " 3.58,\n",
       " 2.08,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 0.16,\n",
       " 1.28,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 2.08,\n",
       " 3.58,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 4.58,\n",
       " 3.58,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 3.58,\n",
       " 0.64,\n",
       " 0.16,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 4.58,\n",
       " 1.92,\n",
       " 3.58,\n",
       " 0.8,\n",
       " 0.16,\n",
       " 4.58,\n",
       " 0.64,\n",
       " 3.14,\n",
       " 2.08,\n",
       " 0.8,\n",
       " 2.08,\n",
       " 4.58,\n",
       " 4.58,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.8,\n",
       " 2.08,\n",
       " 2.08,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 1.92,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 2.78,\n",
       " 0.64,\n",
       " 0.8,\n",
       " 0.8,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 1.92,\n",
       " 2.08,\n",
       " 0.8,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 0.8,\n",
       " 0.16,\n",
       " 0.64,\n",
       " 1.92,\n",
       " 2.08,\n",
       " 2.08,\n",
       " 6.08,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 3.58,\n",
       " 0.64,\n",
       " 0.8,\n",
       " 0.64,\n",
       " 6.08,\n",
       " 2.08,\n",
       " 2.14,\n",
       " 4.58,\n",
       " 1.92,\n",
       " 2.08,\n",
       " 1.92,\n",
       " 0.8,\n",
       " 2.08,\n",
       " 3.58,\n",
       " 4.58,\n",
       " 1.92,\n",
       " 0.64,\n",
       " 1.92,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 6.08,\n",
       " 0.64,\n",
       " 1.28,\n",
       " 0.64,\n",
       " 6.08,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 0.8,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 4.58,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 1.92,\n",
       " 0.8,\n",
       " 0.64,\n",
       " 6.08,\n",
       " 3.58,\n",
       " 0.64,\n",
       " 3.58,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 0.8,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 4.58,\n",
       " 0.64,\n",
       " 1.92,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 0.8,\n",
       " 7.58,\n",
       " 2.08,\n",
       " 3.58,\n",
       " 0.64,\n",
       " 4.58,\n",
       " 0.64,\n",
       " 3.58,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.8,\n",
       " 0.64,\n",
       " 1.28,\n",
       " 0.64,\n",
       " 1.92,\n",
       " 3.14,\n",
       " 0.8,\n",
       " 4.58,\n",
       " 2.08,\n",
       " 2.08,\n",
       " 2.08,\n",
       " 0.8,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 4.42,\n",
       " 2.08,\n",
       " 1.92,\n",
       " 0.8,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 4.58,\n",
       " 0.64,\n",
       " 0.8,\n",
       " 0.8,\n",
       " 0.8,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.8,\n",
       " 0.8,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 4.58,\n",
       " 1.28,\n",
       " 2.08,\n",
       " 0.8,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 6.08,\n",
       " 2.14,\n",
       " 0.64,\n",
       " 1.5,\n",
       " 6.08,\n",
       " 1.92,\n",
       " 2.08,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 4.58,\n",
       " 3.58,\n",
       " 0.64,\n",
       " 0.16,\n",
       " 0.64,\n",
       " 0.8,\n",
       " 2.08,\n",
       " 1.92,\n",
       " 3.42,\n",
       " 6.08,\n",
       " 2.08,\n",
       " 3.58,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 6.08,\n",
       " 0.8,\n",
       " 0.8,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 1.5,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 3.58,\n",
       " 0.16,\n",
       " 2.08,\n",
       " 6.08,\n",
       " 2.08,\n",
       " 3.3,\n",
       " 2.14,\n",
       " 3.58,\n",
       " 4.8,\n",
       " 2.08,\n",
       " 2.08,\n",
       " 0.16,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 3.58,\n",
       " 0.8,\n",
       " 1.44,\n",
       " 0.64,\n",
       " 3.58,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 2.14,\n",
       " 0.64,\n",
       " 0.8,\n",
       " 0.64,\n",
       " 1.92,\n",
       " 2.08,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 0.16,\n",
       " 1.44,\n",
       " 0.8,\n",
       " 1.28,\n",
       " 3.42,\n",
       " 0.64,\n",
       " 0.16,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 3.58,\n",
       " 1.92,\n",
       " 2.08,\n",
       " 3.3,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 0.8,\n",
       " 2.14,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.8,\n",
       " 0.16,\n",
       " 0.64,\n",
       " 1.44,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 2.08,\n",
       " 3.58,\n",
       " 1.92,\n",
       " 1.5,\n",
       " 0.16,\n",
       " 3.3,\n",
       " 1.66,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 5.08,\n",
       " 1.92,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 0.8,\n",
       " 1.5,\n",
       " 0.8,\n",
       " 3.58,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 6.08,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.16,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 3.42,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 6.08,\n",
       " 4.58,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 1.28,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 0.8,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 4.58,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 4.58,\n",
       " 1.28,\n",
       " 0.64,\n",
       " 3.58,\n",
       " 2.08,\n",
       " 1.92,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 2.08,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.8,\n",
       " 2.08,\n",
       " 2.08,\n",
       " 1.92,\n",
       " 0.8,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 0.8,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 1.92,\n",
       " 1.92,\n",
       " 1.44,\n",
       " 0.64,\n",
       " 4.58,\n",
       " 0.8,\n",
       " 1.92,\n",
       " 1.92,\n",
       " 1.92,\n",
       " 0.8,\n",
       " 1.92,\n",
       " 0.64,\n",
       " 0.8,\n",
       " 0.64,\n",
       " 0.8,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 0.8,\n",
       " 0.64,\n",
       " 0.8,\n",
       " 1.92,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 1.92,\n",
       " 0.16,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 2.08,\n",
       " 0.8,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 0.16,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 1.92,\n",
       " 2.08,\n",
       " 2.08,\n",
       " 0.8,\n",
       " 0.64,\n",
       " 1.92,\n",
       " 2.08,\n",
       " 2.08,\n",
       " 1.44,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 1.92,\n",
       " 1.44,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 3.58,\n",
       " 0.64,\n",
       " 1.92,\n",
       " 2.08,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 2.14,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 6.08,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 6.08,\n",
       " 2.08,\n",
       " 0.8,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 0.16,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 1.92,\n",
       " 0.8,\n",
       " 0.64,\n",
       " 0.8,\n",
       " 0.64,\n",
       " 1.28,\n",
       " 0.64,\n",
       " 4.58,\n",
       " 2.08,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 1.44,\n",
       " 6.08,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 1.92,\n",
       " 0.64,\n",
       " 1.5,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 7.58,\n",
       " 0.8,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 6.08,\n",
       " 0.64,\n",
       " 1.44,\n",
       " 0.16,\n",
       " 5.08,\n",
       " 3.58,\n",
       " 0.8,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 1.92,\n",
       " 6.08,\n",
       " 2.08,\n",
       " 4.58,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 1.92,\n",
       " 0.8,\n",
       " 2.08,\n",
       " 3.58,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 3.58,\n",
       " 4.8,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 0.8,\n",
       " 0.64,\n",
       " 1.44,\n",
       " 2.08,\n",
       " 1.28,\n",
       " 0.64,\n",
       " 0.8,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 1.92,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.8,\n",
       " 4.58,\n",
       " 4.58,\n",
       " 2.08,\n",
       " 2.08,\n",
       " 3.58,\n",
       " 0.8,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 0.8,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.8,\n",
       " 2.08,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 1.92,\n",
       " 1.28,\n",
       " 2.08,\n",
       " 3.58,\n",
       " 1.28,\n",
       " 0.64,\n",
       " 1.28,\n",
       " 0.8,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 2.08,\n",
       " 4.58,\n",
       " 2.14,\n",
       " 4.58,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.8,\n",
       " 2.08,\n",
       " 0.8,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 3.58,\n",
       " 2.08,\n",
       " 4.58,\n",
       " 0.16,\n",
       " 3.58,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 3.58,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 1.92,\n",
       " 1.92,\n",
       " 4.58,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 0.8,\n",
       " 0.8,\n",
       " 0.64,\n",
       " 3.58,\n",
       " 0.8,\n",
       " 0.64,\n",
       " 1.44,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 4.58,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 3.58,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 5.08,\n",
       " 0.16,\n",
       " 0.64,\n",
       " 4.58,\n",
       " 0.64,\n",
       " 4.58,\n",
       " 0.16,\n",
       " 2.08,\n",
       " 3.58,\n",
       " 0.64,\n",
       " 0.8,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 3.58,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 1.92,\n",
       " 0.64,\n",
       " 0.8,\n",
       " 2.08,\n",
       " 0.16,\n",
       " 0.8,\n",
       " 1.92,\n",
       " 2.14,\n",
       " 0.64,\n",
       " 3.58,\n",
       " 4.58,\n",
       " 2.08,\n",
       " 1.92,\n",
       " 4.58,\n",
       " 2.14,\n",
       " 0.8,\n",
       " 2.08,\n",
       " 3.58,\n",
       " 0.8,\n",
       " 0.16,\n",
       " 2.14,\n",
       " 0.64,\n",
       " 0.8,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 1.92,\n",
       " 1.28,\n",
       " 0.8,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.8,\n",
       " 6.08,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 6.08,\n",
       " 0.8,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 3.58,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 4.58,\n",
       " 2.3,\n",
       " 1.92,\n",
       " 2.08,\n",
       " 2.08,\n",
       " 0.8,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 1.44,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 4.58,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 2.08,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.8,\n",
       " 3.58,\n",
       " 3.58,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.16,\n",
       " 1.44,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 2.08,\n",
       " 1.92,\n",
       " 1.92,\n",
       " 0.64,\n",
       " 3.58,\n",
       " 4.58,\n",
       " 5.08,\n",
       " 0.64,\n",
       " 1.92,\n",
       " 2.3,\n",
       " 2.08,\n",
       " 2.08,\n",
       " 2.08,\n",
       " 1.92,\n",
       " 0.64,\n",
       " 0.8,\n",
       " 2.08,\n",
       " 4.58,\n",
       " 1.92,\n",
       " 2.08,\n",
       " 0.8,\n",
       " 0.64,\n",
       " 1.5,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.8,\n",
       " 0.8,\n",
       " 2.08,\n",
       " 1.28,\n",
       " 1.92,\n",
       " 1.92,\n",
       " 2.08,\n",
       " 1.28,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 0.8,\n",
       " 2.08,\n",
       " 1.92,\n",
       " 4.58,\n",
       " 2.08,\n",
       " 2.08,\n",
       " 1.92,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 2.08,\n",
       " 2.08,\n",
       " 2.08,\n",
       " 1.92,\n",
       " 3.58,\n",
       " 2.08,\n",
       " 4.58,\n",
       " 0.64,\n",
       " 0.16,\n",
       " 2.08,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.8,\n",
       " 2.08,\n",
       " 2.08,\n",
       " 0.8,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 2.08,\n",
       " 0.8,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 0.8,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 0.8,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 0.64,\n",
       " 1.92,\n",
       " 2.08,\n",
       " 1.92,\n",
       " 0.64,\n",
       " 3.58,\n",
       " 2.08,\n",
       " 0.64,\n",
       " 2.08,\n",
       " 2.08,\n",
       " 2.08,\n",
       " 0.8,\n",
       " 3.58,\n",
       " 0.64,\n",
       " ...]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2708615",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\omar_\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataset.py:342: UserWarning: Length of split at index 1 is 0. This might result in an empty dataset.\n",
      "  warnings.warn(f\"Length of split at index {i} is 0. \"\n"
     ]
    }
   ],
   "source": [
    "ss, ww = x.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61cbe031",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(ss, batch_size=16, shuffle=True)\n",
    "test_dl = DataLoader(ww, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e76429da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x2505bc7d780>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6be8377a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "must be real number, not BatchEncoding",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    145\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcollate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    145\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcollate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0melem_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0melem_type\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mcollate_float_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcollate_float_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mType\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mType\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m...\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: must be real number, not BatchEncoding",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8996\\805884452.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1194\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1195\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1196\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1197\u001b[0m                 \u001b[1;31m# Update and possibly print the progressbar.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    626\u001b[0m                 \u001b[1;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 628\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 671\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    672\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    263\u001b[0m             \u001b[1;33m>>\u001b[0m\u001b[1;33m>\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Handle `CustomType` automatically\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m     \"\"\"\n\u001b[1;32m--> 265\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcollate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdefault_collate_fn_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    147\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m                 \u001b[1;31m# The sequence type may not support `__init__(iterable)` (e.g., `range`).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcollate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    147\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m                 \u001b[1;31m# The sequence type may not support `__init__(iterable)` (e.g., `range`).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcollate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcollate_fn_map\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0melem_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0melem_type\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcollate_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mcollate_float_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcollate_float_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mType\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mType\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m...\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: must be real number, not BatchEncoding"
     ]
    }
   ],
   "source": [
    "TypeError: must be real number, not BatchEncoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893d0acf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
